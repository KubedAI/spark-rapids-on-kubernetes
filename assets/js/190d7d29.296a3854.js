"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[455],{3739:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>p,frontMatter:()=>s,metadata:()=>o,toc:()=>d});var t=a(4848),r=a(8453);const s={sidebar_position:1,sidebar_label:"Data Generation",title:"Spark RAPIDS Benchmark Data Generation Script"},i="Spark RAPIDS Benchmark Data Generation Script \ud83d\ude80",o={id:"Benchmarks/data-generation",title:"Spark RAPIDS Benchmark Data Generation Script",description:"This Spark job generates synthetic retail data for benchmarking Spark RAPIDS performance on Kubernetes. The generated data includes sales, customer, and product information in CSV format and is designed for large-scale testing scenarios. By default, the script generates:",source:"@site/docs/Benchmarks/data-generation.md",sourceDirName:"Benchmarks",slug:"/Benchmarks/data-generation",permalink:"/spark-rapids-on-kubernetes/docs/Benchmarks/data-generation",draft:!1,unlisted:!1,editUrl:"https://github.com/Kube-dAI/spark-rapids-on-kubernetes/tree/main/packages/create-spark-rapids-on-kubernetes/templates/shared/docs/Benchmarks/data-generation.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1,sidebar_label:"Data Generation",title:"Spark RAPIDS Benchmark Data Generation Script"},sidebar:"docSidebar",previous:{title:"Benchmarks",permalink:"/spark-rapids-on-kubernetes/docs/category/benchmarks"},next:{title:"Benchmarks",permalink:"/spark-rapids-on-kubernetes/docs/Benchmarks/"}},c={},d=[{value:"Spark Operator Configuration \u2699\ufe0f",id:"spark-operator-configuration-\ufe0f",level:2},{value:"How to Deploy on Kubernetes",id:"how-to-deploy-on-kubernetes",level:2},{value:"Verifying Output Data \u2705",id:"verifying-output-data-",level:2},{value:"Next Steps: Running Benchmarks",id:"next-steps-running-benchmarks",level:2},{value:"Cleanup",id:"cleanup",level:2}];function l(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",p:"p",pre:"pre",strong:"strong",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"spark-rapids-benchmark-data-generation-script-",children:"Spark RAPIDS Benchmark Data Generation Script \ud83d\ude80"})}),"\n",(0,t.jsxs)(n.p,{children:["This Spark job generates synthetic retail data for benchmarking Spark RAPIDS performance on Kubernetes. The generated data includes sales, customer, and product information in CSV format and is designed for large-scale testing scenarios. By default, the ",(0,t.jsx)(n.a,{href:"https://github.com/KubedAI/spark-rapids-on-kubernetes/blob/main/benchmarks/data-gen/data-generation-retail.yaml",children:"script"})," generates:"]}),"\n",(0,t.jsx)(n.p,{children:"\ud83d\udcca Sales Table: 1 billion rows, generating 60GB of sales data."}),"\n",(0,t.jsx)(n.p,{children:"\ud83d\udc65 Customer Table: 100 million rows, creating 6GB of customer data."}),"\n",(0,t.jsx)(n.p,{children:"\ud83d\uded2 Product Table: 100k rows, creating 5MB of product data."}),"\n",(0,t.jsx)(n.h2,{id:"spark-operator-configuration-\ufe0f",children:"Spark Operator Configuration \u2699\ufe0f"}),"\n",(0,t.jsx)(n.p,{children:"Below is a sample Spark Operator YAML configuration for submitting the data generation job on a Kubernetes cluster. This job runs the PySpark script stored in an S3 bucket and generates data that is saved back to S3."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'apiVersion: "sparkoperator.k8s.io/v1beta2"\nkind: SparkApplication\nmetadata:\n  name: spark-data-generation\n  namespace: data-eng-team\nspec:\n  type: Python\n  pythonVersion: "3"\n  mode: cluster\n  image: kubedai/spark:spark-3.5.1_hadoop-3.3.4_aws-sdk-1.12.773\n  imagePullPolicy: Always\n  mainApplicationFile: "s3a://<S3_BUCKET>/benchmark/scripts/data-generation-retail.py"\n  arguments:\n    - "s3a://<S3_BUCKET>/benchmark/input"\n    - "1000000000"  # Sales table (60GB)\n    - "100000000"   # Customer table (6GB)\n    - "100000"      # Product table (5MB)\n  sparkVersion: "3.3.1"\n  ...\n\n'})}),"\n",(0,t.jsx)(n.h2,{id:"how-to-deploy-on-kubernetes",children:"How to Deploy on Kubernetes"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Step1: Copy the PySpark Script to S3"})}),"\n",(0,t.jsxs)(n.p,{children:["Upload the PySpark script to the S3 bucket (e.g., ",(0,t.jsx)(n.code,{children:"s3://<S3_BUCKET>/benchmark/scripts/data-generation-retail.py"}),"). The S3 bucket is automatically created during the cluster deployment using Terraform templates. To find the bucket name, run ",(0,t.jsx)(n.code,{children:"terraform outputs"})," from the ",(0,t.jsx)(n.code,{children:"infra/aws/terraform"})," directory."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"aws s3 cp benchmarks/dat-gen/data-generation-retail.py s3://<S3_BUCKET>/benchmark/scripts/\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Step2: Execute Spark Job \ud83d\udea6"})}),"\n",(0,t.jsxs)(n.p,{children:["Make sure you replace ",(0,t.jsx)(n.code,{children:"<S3_BUCKET>"})," in the yaml with the actual bucket name before executing the following command."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl apply -f spark-operator-job.yaml\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Step3: Monitor the Job \ud83d\udd0d"})}),"\n",(0,t.jsx)(n.p,{children:"Track the job's progress by running the following commands:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl logs -f spark-data-gen -n data-eng-team\n"})}),"\n",(0,t.jsx)(n.h2,{id:"verifying-output-data-",children:"Verifying Output Data \u2705"}),"\n",(0,t.jsxs)(n.p,{children:["Once the job completes, verify that the generated data has been written to the S3 bucket specified in the script arguments. The output data will be stored in CSV format at the path ",(0,t.jsx)(n.code,{children:"s3a://<S3_BUCKET>/benchmark/input/"}),"."]}),"\n",(0,t.jsx)(n.p,{children:"Use the following command to list the files in the S3 bucket:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"aws s3 ls s3://<S3_BUCKET>/benchmark/input/\n"})}),"\n",(0,t.jsx)(n.h2,{id:"next-steps-running-benchmarks",children:"Next Steps: Running Benchmarks"}),"\n",(0,t.jsx)(n.p,{children:"The data generated by this job will be used as the input dataset for running Spark RAPIDS benchmarks. For details on running the benchmarks, refer to the benchmark guide."}),"\n",(0,t.jsx)(n.h2,{id:"cleanup",children:"Cleanup"}),"\n",(0,t.jsx)(n.p,{children:"To delete the job and clean up resources:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"kubectl delete -f spark-operator-job.yaml\n"})})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>i,x:()=>o});var t=a(6540);const r={},s=t.createContext(r);function i(e){const n=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);