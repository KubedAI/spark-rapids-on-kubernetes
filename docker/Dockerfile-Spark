# Use the official Apache Spark base image
FROM apache/spark:3.5.1

# Define versions for Spark, Hadoop, and AWS SDK
ARG SPARK_VERSION=3.5.1
ARG HADOOP_VERSION=3.3.4
ARG AWS_SDK_VERSION=1.12.773

# Set environment variables for Spark, Hadoop, and AWS SDK versions
ENV SPARK_VERSION=${SPARK_VERSION}
ENV HADOOP_VERSION=${HADOOP_VERSION}
ENV AWS_SDK_VERSION=${AWS_SDK_VERSION}

# Install necessary packages: wget, Python, PySpark, and numpy
USER root
RUN apt-get update && \
    apt-get install -y wget python3 python3-pip && \
    ln -s /usr/bin/python3 /usr/bin/python && \
    pip3 install pyspark==$SPARK_VERSION numpy && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Download AWS Java SDK and Hadoop-AWS library to enable S3A support
# These libraries allow Spark to interact with AWS S3 storage using the S3A protocol
RUN cd /opt/spark/jars && \
    wget -q "https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/${AWS_SDK_VERSION}/aws-java-sdk-bundle-${AWS_SDK_VERSION}.jar" && \
    wget -q "https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/${HADOOP_VERSION}/hadoop-aws-${HADOOP_VERSION}.jar"

# Switch back to non-root user for security best practices
USER 1001

# Set the entry point for the Spark container
ENTRYPOINT ["/opt/entrypoint.sh"]


